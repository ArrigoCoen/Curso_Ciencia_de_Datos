{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House prices with Linear, Lasso and Ridge regressions using Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook trabajaremos el uso de `Pipelines` para estimar los precios de casas utilizando `Lineal`, `Lasso` y `Ridge` regressions.\n",
    "\n",
    "Los datos que aquí trabajamos pueden ser bajados de [Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se cargan las librerías que se van a utilizar en ambos ejemplos\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# OBSERVACI´ON:\n",
    "# Otra posible mejora de este modelo podría ser utilizando regresión polinómica en combinación a Linear, Lasso o \n",
    "# Ridge. \n",
    "from sklearn.preprocessing import PolynomialFeatures  # <------ library to perform Polynomial Regression\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.preprocessing import scale \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "pd.set_option('display.max_rows', 90) # by default is 10, if change to None print ALL\n",
    "pd.set_option('display.max_columns', 90) # by default is 10, if change to None print ALL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargado de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante notar que tenemos dos archivos:\n",
    "\n",
    " - `train.csv` contiene los datos que utilizaremos para encontrar el modelo adecuado; en particular, contiene una muestra de `X` con sus respectivas `y`.\n",
    " - `test.csv` contiene los datos que utilizaremos para hacer nuetra `submission` la página de Kaggle; en particular, contiene una muestra de `X` PERO NO TIENE LAS RESPECTIVAS `y`.\n",
    " \n",
    "Los datos del archivo `train.csv` los separaremos en `X_train`, `X_test`, `y_train` y `y_test` para ajustar y probar nuestro modelo. No utilizamos los datos del archivo `test.csv` para encontrar nuestro modelo ya que no contiene la variable `y` correspondiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1) EXTRAER DATOS\n",
    "# Los datos pueden encontrarse en diferentes formatos, en nuestro caso están en formato csv.\n",
    "\n",
    "# Se carga la base de datos\n",
    "train = pd.read_csv('train.csv') #Se encuentra en la misma carpeta que el jupyter notebook\n",
    "test = pd.read_csv('test.csv') #Se encuentra en la misma carpeta que el jupyter notebook\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos notar que las nuevas dataframe que hicimos (`train` y `test`) son de diferentes tamaños, ya que `test` no tiene las variable dependiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape) \n",
    "print(test.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliminate of some columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos a eleminar las columnas que tienen más del 50% de sus valores como nulas en train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_plus_50percent_null = train.isnull().sum()[train.isnull().sum()>train.shape[0]/2]\n",
    "col_plus_50percent_null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observemos que también hay casi las mismas columnas en test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.isnull().sum()[test.isnull().sum()>test.shape[0]/2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces nos queda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_drop = ['PoolQC','MiscFeature','Alley','Fence']\n",
    "train = train.drop(features_drop, axis=1)\n",
    "test  =  test.drop(features_drop, axis=1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprovemos que ya no tenemos esas variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_plus_50percent_null = train.isnull().sum()[train.isnull().sum()>train.shape[0]/2]\n",
    "col_plus_50percent_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.isnull().sum()[test.isnull().sum()>test.shape[0]/2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separación de variables\n",
    "\n",
    "Separemos las variables en `X_train`, `X_test`, `y_train`, `y_test`, al igual que elijamos que columnas son numericas, ordinales y nominales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numerical = train.select_dtypes(include=np.number).columns.tolist()\n",
    "numerical.remove('Id')\n",
    "numerical.remove('SalePrice')\n",
    "\n",
    "nominal = train.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "# OBSERVACION: \n",
    "# Para mejorar el modelo podríamos utilizar agregar variables ordinales a para mejorar el ajuste.\n",
    "# Un ejemplo de variables ordinales es el siguiente:\n",
    "# \n",
    "# ordinal = [\"LandSlope\", \"OverallQual\", \"OverallCond\", \"YearRemodAdd\",\n",
    "#           \"ExterQual\", \"ExterCond\", \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\",\n",
    "#           \"KitchenQual\", \"Functional\", \"GarageCond\", \"PavedDrive\"]\n",
    "\n",
    "ordinal = []\n",
    "\n",
    "X = train[nominal + ordinal + numerical] #LotFrontage y MasVnrType tiene NaNs\n",
    "y = train['SalePrice']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) \n",
    "\n",
    "X_REAL_test = test[nominal + ordinal + numerical]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines auxiliares\n",
    "\n",
    "Para separar mejor el procesamiento de nuestros datos, utilizamos tres pipelines auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline datos ordinales\n",
    "ordinal_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", OrdinalEncoder())\n",
    "])\n",
    "\n",
    "# Pipeline datos nominales\n",
    "nominal_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", OneHotEncoder(sparse=True, handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "# Pipeline datos numéricos\n",
    "numerical_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "# Pegado de los tres pipelines\n",
    "preprocessing_pipeline = ColumnTransformer([\n",
    "    (\"nominal_preprocessor\", nominal_pipeline, nominal),\n",
    "    (\"ordinal_preprocessor\", ordinal_pipeline, ordinal),\n",
    "    (\"numerical_preprocessor\", numerical_pipeline, numerical)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente agregamos todo en un solo pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed_features = preprocessing_pipeline.fit_transform(train_features)\n",
    "\n",
    "# ML_model = Lasso(alpha=190)\n",
    "# ML_model = Ridge(alpha=20)\n",
    "ML_model = LinearRegression()\n",
    "\n",
    "complete_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessing_pipeline),\n",
    "    (\"estimator\", ML_model)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_pipeline.fit(X_train, y_train)\n",
    "y_pred = complete_pipeline.predict(X_test)\n",
    "\n",
    "print('ERRORS OF PREDICTIONS')\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, y_pred)) \n",
    "print('MSE:', metrics.mean_squared_error(y_test, y_pred)) \n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred))) \n",
    "print('r2_score:', r2_score(y_test, y_pred)) \n",
    "\n",
    "p1 = max(max(y_pred), max(y_test))\n",
    "p2 = min(min(y_pred), min(y_test))\n",
    "plt.plot([p1, p2], [p1, p2], 'b-')\n",
    "plt.scatter(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generación de archivo para Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_REAL_test = complete_pipeline.predict(X_REAL_test)\n",
    "\n",
    "pred=pd.DataFrame(y_REAL_test)\n",
    "sub_df=pd.read_csv('sample_submission.csv')\n",
    "datasets=pd.concat([sub_df['Id'],pred],axis=1)\n",
    "datasets.columns=['Id','SalePrice']\n",
    "datasets.to_csv('sample_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para subir el archivo es [aquí](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview/evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Búsqueda de hiperparámetros\n",
    "A continuación mostramos como fue que eligimos los parámetros `alpha` para los modelos `Lasso` y `Ridge` utilizando el comando `GridSearchCV`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encontrando alpha de Lasso (alpha = 180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters={'alpha':[100,150,170,180,190,200,220,250,300]}\n",
    "ML_model=Lasso()\n",
    "grid = GridSearchCV(ML_model,parameters,scoring='neg_mean_squared_error',cv=5)\n",
    "grid.fit(preprocessing_pipeline.fit_transform(X_train),y_train)\n",
    "# Convert the results of CV into a dataframe\n",
    "results = pd.DataFrame(grid.cv_results_)[['params', 'mean_test_score', 'rank_test_score']]\n",
    "results.sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encontrando alpha de Ridge (alpha = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters={'alpha':[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,20,30,35,40,45,50,55,100,200,300]}\n",
    "ML_model=Ridge()\n",
    "grid = GridSearchCV(ML_model,parameters,scoring='neg_mean_squared_error',cv=5)\n",
    "grid.fit(preprocessing_pipeline.fit_transform(X_train),y_train)\n",
    "# Convert the results of CV into a dataframe\n",
    "results = pd.DataFrame(grid.cv_results_)[['params', 'mean_test_score', 'rank_test_score']]\n",
    "results.sort_values('rank_test_score')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
